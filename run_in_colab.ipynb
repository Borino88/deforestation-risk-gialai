{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb3j958HvZBq",
        "outputId": "d4c80a25-aced-491f-955e-e77e62c34b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project created at: /content/deforestation_risk_project\n"
          ]
        }
      ],
      "source": [
        "!pip -q install pandas numpy scikit-learn matplotlib\n",
        "\n",
        "import os, textwrap, pathlib\n",
        "from datetime import datetime\n",
        "\n",
        "PROJECT_DIR = \"/content/deforestation_risk_project\"\n",
        "SRC_DIR = f\"{PROJECT_DIR}/src\"\n",
        "OUT_DIR = f\"{PROJECT_DIR}/outputs\"\n",
        "for p in [SRC_DIR, f\"{OUT_DIR}/tables\", f\"{OUT_DIR}/figures\", f\"{OUT_DIR}/predictions\"]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"Project created at:\", PROJECT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_code = r'''\n",
        "import os, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score,\n",
        "    precision_score, recall_score, f1_score,\n",
        "    precision_recall_curve, roc_curve\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _rename_ci(df, targets):\n",
        "    lower = {c.lower(): c for c in df.columns}\n",
        "    for t in targets:\n",
        "        if t not in df.columns and t.lower() in lower:\n",
        "            df = df.rename(columns={lower[t.lower()]: t})\n",
        "    return df\n",
        "\n",
        "def _best_f1_threshold(y_true, scores):\n",
        "    prec, rec, thr = precision_recall_curve(y_true, scores)\n",
        "    f1 = (2*prec*rec)/(prec+rec+1e-12)\n",
        "    i = int(np.nanargmax(f1))\n",
        "    if i == 0 or i-1 >= len(thr):\n",
        "        return 0.5\n",
        "    return float(thr[i-1])\n",
        "\n",
        "def _capture_rate(y_true, scores, top_frac):\n",
        "    y_true = np.asarray(y_true)\n",
        "    scores = np.asarray(scores)\n",
        "    k = max(1, int(math.ceil(top_frac * len(y_true))))\n",
        "    idx = np.argsort(-scores)[:k]\n",
        "    return float(y_true[idx].sum() / max(1, y_true.sum()))\n",
        "\n",
        "def _save_roc(y_true, score_dict, out_path, title):\n",
        "    plt.figure(figsize=(7,5))\n",
        "    for name, sc in score_dict.items():\n",
        "        fpr, tpr, _ = roc_curve(y_true, sc)\n",
        "        plt.plot(fpr, tpr, label=name)\n",
        "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "def run_pipeline(\n",
        "    kbang_train_path,\n",
        "    kbang_test_path,\n",
        "    mang_train_path,\n",
        "    mang_test_path,\n",
        "    out_dir\n",
        "):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    tables_dir = os.path.join(out_dir, \"tables\")\n",
        "    figs_dir = os.path.join(out_dir, \"figures\")\n",
        "    preds_dir = os.path.join(out_dir, \"predictions\")\n",
        "    for p in [tables_dir, figs_dir, preds_dir]:\n",
        "        os.makedirs(p, exist_ok=True)\n",
        "\n",
        "    # 1) Load\n",
        "    dfs = {\n",
        "        \"KBang_train\": pd.read_csv(kbang_train_path),\n",
        "        \"KBang_test\": pd.read_csv(kbang_test_path),\n",
        "        \"MangYang_train\": pd.read_csv(mang_train_path),\n",
        "        \"MangYang_test\": pd.read_csv(mang_test_path),\n",
        "    }\n",
        "    for k in dfs:\n",
        "        dfs[k].columns = [c.strip() for c in dfs[k].columns]\n",
        "\n",
        "    def add_required(df, district, split):\n",
        "        df = df.copy()\n",
        "        df[\"district\"] = district\n",
        "        df[\"split\"] = split\n",
        "\n",
        "        # lon/lat normalization\n",
        "        for c in [\"lon\",\"lat\"]:\n",
        "            if c not in df.columns and c.upper() in df.columns:\n",
        "                df = df.rename(columns={c.upper(): c})\n",
        "\n",
        "        # label_std fallback\n",
        "        if \"label_std\" not in df.columns:\n",
        "            if \"label\" in df.columns:\n",
        "                df[\"label_std\"] = df[\"label\"]\n",
        "            elif \"defo\" in df.columns:\n",
        "                df[\"label_std\"] = df[\"defo\"]\n",
        "            elif \"loss_any_2001_2024\" in df.columns:\n",
        "                df[\"label_std\"] = df[\"loss_any_2001_2024\"]\n",
        "            else:\n",
        "                df[\"label_std\"] = np.nan\n",
        "\n",
        "        df[\"label_std\"] = pd.to_numeric(df[\"label_std\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "        return df\n",
        "\n",
        "    combined = pd.concat([\n",
        "        add_required(dfs[\"KBang_train\"], \"KBang\", \"train\"),\n",
        "        add_required(dfs[\"KBang_test\"], \"KBang\", \"test\"),\n",
        "        add_required(dfs[\"MangYang_train\"], \"MangYang\", \"train\"),\n",
        "        add_required(dfs[\"MangYang_test\"], \"MangYang\", \"test\"),\n",
        "    ], ignore_index=True)\n",
        "\n",
        "    targets = [\n",
        "        \"lon\",\"lat\",\"treecover2000\",\"mean_elevation_m\",\"mean_slope_deg\",\n",
        "        \"rain_last12m_total_mm\",\"rain_mean_annual_2000_2024_mm\",\n",
        "        \"loss_any_2001_2024\",\"loss_first_year\"\n",
        "    ]\n",
        "    combined = _rename_ci(combined, targets)\n",
        "\n",
        "    # 2) QC Summary\n",
        "    qc = {\n",
        "        \"rows\": int(len(combined)),\n",
        "        \"cols\": int(combined.shape[1]),\n",
        "        \"district_counts\": combined[\"district\"].value_counts().to_dict(),\n",
        "        \"split_counts\": combined[\"split\"].value_counts().to_dict(),\n",
        "        \"pos_rate_label_std\": float(combined[\"label_std\"].mean()),\n",
        "    }\n",
        "    core_cols = [c for c in targets if c in combined.columns]\n",
        "    qc[\"missing_core\"] = {c: int(combined[c].isna().sum()) for c in core_cols}\n",
        "    if \"lon\" in combined.columns and \"lat\" in combined.columns:\n",
        "        qc[\"duplicate_lonlat_rows\"] = int(combined.duplicated(subset=[\"lon\",\"lat\"]).sum())\n",
        "    pd.DataFrame([qc]).to_csv(os.path.join(out_dir, \"QC_summary.csv\"), index=False)\n",
        "\n",
        "    # Save final master\n",
        "    combined.to_csv(os.path.join(out_dir, \"ALL_master_FINAL.csv\"), index=False)\n",
        "\n",
        "    # 3) Table 1 (Datasets)\n",
        "    table1 = pd.DataFrame([\n",
        "        [\"Hansen Global Forest Change\", \"treecover2000 + lossyear-derived labels\", \"30 m\", \"2000 baseline; loss 2001–2024\", \"Forest baseline + historical loss label\"],\n",
        "        [\"SRTM\", \"mean_elevation_m + mean_slope_deg\", \"30–90 m\", \"Static\", \"Terrain predictors\"],\n",
        "        [\"CHIRPS\", \"rain_last12m_total_mm + rain_mean_annual_2000_2024_mm\", \"~5 km\", \"2000–2025\", \"Rainfall predictors\"],\n",
        "    ], columns=[\"Dataset\",\"Layers/Variables\",\"Native resolution\",\"Time span\",\"Role\"])\n",
        "    table1.to_csv(os.path.join(tables_dir, \"Table1_datasets.csv\"), index=False)\n",
        "\n",
        "    # 4) Table 2 (Dataset Summary by district)\n",
        "    def summarize(group):\n",
        "        out = {\n",
        "            \"n_samples\": len(group),\n",
        "            \"n_train\": int((group[\"split\"]==\"train\").sum()),\n",
        "            \"n_test\": int((group[\"split\"]==\"test\").sum()),\n",
        "            \"pos_rate_label_std\": float(group[\"label_std\"].mean()),\n",
        "        }\n",
        "        for col in [\"treecover2000\",\"mean_elevation_m\",\"mean_slope_deg\",\"rain_last12m_total_mm\",\"rain_mean_annual_2000_2024_mm\",\"loss_first_year\"]:\n",
        "            if col in group.columns:\n",
        "                out[f\"{col}_mean\"] = float(group[col].mean())\n",
        "                out[f\"{col}_std\"] = float(group[col].std())\n",
        "        return pd.Series(out)\n",
        "\n",
        "    table2 = combined.groupby(\"district\").apply(summarize).reset_index()\n",
        "    table2.to_csv(os.path.join(tables_dir, \"Table2_dataset_summary.csv\"), index=False)\n",
        "\n",
        "    # 5) Modeling\n",
        "    meta = {\"district\",\"split\",\"lon\",\"lat\",\"label_std\",\"label\",\"defo\"}\n",
        "    label_related = {\"loss_any_2001_2024\",\"loss_first_year\"}\n",
        "    feature_cols = [\n",
        "        c for c in combined.columns\n",
        "        if c not in meta and c not in label_related and pd.api.types.is_numeric_dtype(combined[c])\n",
        "    ]\n",
        "    X = combined[feature_cols].copy().apply(lambda s: s.fillna(s.median()), axis=0)\n",
        "    y = combined[\"label_std\"].astype(int)\n",
        "\n",
        "    # Baseline score (simple)\n",
        "    baseline_cols = [c for c in [\"treecover2000\",\"mean_elevation_m\",\"mean_slope_deg\",\"rain_last12m_total_mm\"] if c in X.columns]\n",
        "    baseX = X[baseline_cols].copy()\n",
        "    if \"mean_elevation_m\" in baseX.columns: baseX[\"mean_elevation_m\"] = -baseX[\"mean_elevation_m\"]\n",
        "    if \"mean_slope_deg\" in baseX.columns: baseX[\"mean_slope_deg\"] = -baseX[\"mean_slope_deg\"]\n",
        "    baseline_prob = 1/(1+np.exp(-StandardScaler().fit_transform(baseX).mean(axis=1)))\n",
        "\n",
        "    logreg = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
        "    ])\n",
        "\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=400,\n",
        "        random_state=42,\n",
        "        class_weight=\"balanced_subsample\",\n",
        "        max_features=\"sqrt\",\n",
        "        max_depth=22,\n",
        "        min_samples_leaf=2,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    def eval_scenario(train_idx, test_idx, scenario_name):\n",
        "        y_tr, y_te = y.iloc[train_idx].values, y.iloc[test_idx].values\n",
        "        X_tr, X_te = X.iloc[train_idx], X.iloc[test_idx]\n",
        "\n",
        "        logreg.fit(X_tr, y_tr)\n",
        "        rf.fit(X_tr, y_tr)\n",
        "\n",
        "        base_tr, base_te = baseline_prob[train_idx], baseline_prob[test_idx]\n",
        "        lr_tr, lr_te = logreg.predict_proba(X_tr)[:,1], logreg.predict_proba(X_te)[:,1]\n",
        "        rf_tr, rf_te = rf.predict_proba(X_tr)[:,1], rf.predict_proba(X_te)[:,1]\n",
        "\n",
        "        thr_base = _best_f1_threshold(y_tr, base_tr)\n",
        "        thr_lr   = _best_f1_threshold(y_tr, lr_tr)\n",
        "        thr_rf   = _best_f1_threshold(y_tr, rf_tr)\n",
        "\n",
        "        rows = []\n",
        "        for model_name, sc, thr in [\n",
        "            (\"Baseline\", base_te, thr_base),\n",
        "            (\"LogReg\", lr_te, thr_lr),\n",
        "            (\"RF\", rf_te, thr_rf),\n",
        "        ]:\n",
        "            rows.append([\n",
        "                scenario_name,\n",
        "                model_name,\n",
        "                roc_auc_score(y_te, sc),\n",
        "                average_precision_score(y_te, sc),\n",
        "                precision_score(y_te, (sc>=thr).astype(int), zero_division=0),\n",
        "                recall_score(y_te, (sc>=thr).astype(int), zero_division=0),\n",
        "                f1_score(y_te, (sc>=thr).astype(int), zero_division=0),\n",
        "                _capture_rate(y_te, sc, 0.05),\n",
        "                _capture_rate(y_te, sc, 0.10),\n",
        "                thr,\n",
        "                len(y_te),\n",
        "                int(y_te.sum())\n",
        "            ])\n",
        "\n",
        "        perf = pd.DataFrame(rows, columns=[\n",
        "            \"Scenario\",\"Model\",\"AUC\",\"AP\",\"Precision\",\"Recall\",\"F1\",\n",
        "            \"Capture@5%\",\"Capture@10%\",\"Threshold(F1-train)\",\"N_test\",\"N_pos_test\"\n",
        "        ])\n",
        "        scores = {\"Baseline\": base_te, \"LogReg\": lr_te, \"RF\": rf_te}\n",
        "        return perf, y_te, scores\n",
        "\n",
        "    # A) provided split\n",
        "    train_idx = combined.index[combined[\"split\"]==\"train\"].to_numpy()\n",
        "    test_idx  = combined.index[combined[\"split\"]==\"test\"].to_numpy()\n",
        "    t3_split, y_split, scores_split = eval_scenario(train_idx, test_idx, \"Split (provided)\")\n",
        "\n",
        "    # B) area transfer KBang -> MangYang\n",
        "    kbang_idx = combined.index[combined[\"district\"]==\"KBang\"].to_numpy()\n",
        "    mang_idx  = combined.index[combined[\"district\"]==\"MangYang\"].to_numpy()\n",
        "    t3_k2m, y_k2m, scores_k2m = eval_scenario(kbang_idx, mang_idx, \"Area transfer (KBang->MangYang)\")\n",
        "\n",
        "    table3 = pd.concat([t3_split, t3_k2m], ignore_index=True)\n",
        "    table3.to_csv(os.path.join(tables_dir, \"Table3_model_performance.csv\"), index=False)\n",
        "\n",
        "    # ROC figures\n",
        "    _save_roc(y_split, scores_split, os.path.join(figs_dir, \"Figure_ROC_split.png\"), \"ROC (provided split)\")\n",
        "    _save_roc(y_k2m, scores_k2m, os.path.join(figs_dir, \"Figure_ROC_KBang_to_MangYang.png\"), \"ROC (KBang->MangYang)\")\n",
        "\n",
        "    # 6) Explainability figures (train on provided split)\n",
        "    logreg.fit(X.iloc[train_idx], y.iloc[train_idx].values)\n",
        "    rf.fit(X.iloc[train_idx], y.iloc[train_idx].values)\n",
        "\n",
        "    importances = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False).head(12)\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.barh(list(reversed(importances.index)), list(reversed(importances.values)))\n",
        "    plt.title(\"RF feature importance (top 12)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(figs_dir, \"Figure_RF_importance.png\"), dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    coef = pd.Series(logreg.named_steps[\"clf\"].coef_[0], index=feature_cols).sort_values()\n",
        "    coef_plot = pd.concat([coef.head(8), coef.tail(8)])\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.barh(coef_plot.index, coef_plot.values)\n",
        "    plt.title(\"LogReg coefficients (most negative/positive)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(figs_dir, \"Figure_LogReg_coefficients.png\"), dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    # 7) Predictions + warning zones\n",
        "    logreg.fit(X, y.values)\n",
        "    rf.fit(X, y.values)\n",
        "\n",
        "    pred = combined.copy()\n",
        "    pred[\"risk_baseline\"] = baseline_prob\n",
        "    pred[\"risk_logreg\"] = logreg.predict_proba(X)[:,1]\n",
        "    pred[\"risk_rf\"] = rf.predict_proba(X)[:,1]\n",
        "\n",
        "    pred[\"warn_top5_rf\"] = 0\n",
        "    pred[\"warn_top10_rf\"] = 0\n",
        "    for d in pred[\"district\"].unique():\n",
        "        sub = pred[pred[\"district\"] == d]\n",
        "        thr5 = np.quantile(sub[\"risk_rf\"], 0.95)\n",
        "        thr10 = np.quantile(sub[\"risk_rf\"], 0.90)\n",
        "        pred.loc[(pred[\"district\"]==d) & (pred[\"risk_rf\"]>=thr5), \"warn_top5_rf\"] = 1\n",
        "        pred.loc[(pred[\"district\"]==d) & (pred[\"risk_rf\"]>=thr10), \"warn_top10_rf\"] = 1\n",
        "\n",
        "    pred.to_csv(os.path.join(preds_dir, \"Model_predictions_and_warning_zones.csv\"), index=False)\n",
        "\n",
        "    # Run log\n",
        "    with open(os.path.join(out_dir, \"README_run_log.txt\"), \"w\") as f:\n",
        "        f.write(\"Pipeline run complete.\\n\")\n",
        "        f.write(f\"Rows: {len(combined)}\\n\")\n",
        "        f.write(f\"Features used: {len(feature_cols)}\\n\")\n",
        "\n",
        "    return {\n",
        "        \"out_dir\": out_dir,\n",
        "        \"feature_cols\": feature_cols,\n",
        "        \"baseline_cols\": baseline_cols\n",
        "    }\n",
        "'''\n",
        "open(\"/content/deforestation_risk_project/src/pipeline.py\",\"w\").write(pipeline_code)\n",
        "print(\"Wrote pipeline to:\", \"/content/deforestation_risk_project/src/pipeline.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_2HSFazwO6C",
        "outputId": "d805feaf-a9f0-4ef3-ad9e-928eaaa2464a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote pipeline to: /content/deforestation_risk_project/src/pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deforestation_risk_project.src.pipeline import run_pipeline\n",
        "\n",
        "result = run_pipeline(\n",
        "    \"/content/KBang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "    \"/content/KBang_TEST_master_rain_elev_lossyear.csv\",\n",
        "    \"/content/MangYang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "    \"/content/MangYang_TEST_master_rain_elev_lossyear.csv\",\n",
        "    \"/content/deforestation_risk_project/outputs\"\n",
        ")\n",
        "\n",
        "print(\"DONE. Outputs at:\", result[\"out_dir\"])\n",
        "print(\"Feature columns used:\", len(result[\"feature_cols\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "hM9NoCahwTXJ",
        "outputId": "7973491a-5d53-4dc0-b4e6-9f4e5bbe154b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/KBang_TRAIN_master_rain_elev_lossyear.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2365741464.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeforestation_risk_project\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m result = run_pipeline(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"/content/KBang_TRAIN_master_rain_elev_lossyear.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"/content/KBang_TEST_master_rain_elev_lossyear.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deforestation_risk_project/src/pipeline.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(kbang_train_path, kbang_test_path, mang_train_path, mang_test_path, out_dir)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# 1) Load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     dfs = {\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;34m\"KBang_train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkbang_train_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;34m\"KBang_test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkbang_test_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;34m\"MangYang_train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmang_train_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/KBang_TRAIN_master_rain_elev_lossyear.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "zip_path = shutil.make_archive(\"/content/outputs_package\", \"zip\", \"/content/deforestation_risk_project/outputs\")\n",
        "print(\"Created:\", zip_path)\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "ynv3fqpowZID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.pipeline import run_pipeline\n",
        "\n",
        "run_pipeline(\n",
        "  \"/content/KBang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/KBang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/outputs\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "-3oZdFHt2hpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Borino88/deforestation-risk-gialai.git\n",
        "%cd deforestation-risk-gialai\n",
        "!pip -q install -r requirements.txt\n",
        "!ls\n",
        "!ls src\n"
      ],
      "metadata": {
        "id": "GR6mzMfv2jHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.pipeline import run_pipeline\n",
        "\n",
        "run_pipeline(\n",
        "  \"/content/KBang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/KBang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/outputs\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Cehr0EUQ3Lsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "zip_path = shutil.make_archive(\"/content/outputs\", \"zip\", \"/content/outputs\")\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "Co6pNjaO3WEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.pipeline import run_pipeline\n",
        "\n",
        "run_pipeline(\n",
        "  \"/content/KBang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/KBang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/outputs\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "qmRkfkb33YHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "zip_path = shutil.make_archive(\"/content/outputs\", \"zip\", \"/content/outputs\")\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "MGftAqF3GuJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Borino88/deforestation-risk-gialai.git\n",
        "%cd deforestation-risk-gialai\n",
        "!pip -q install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "3WzbF1GtG1U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "sys.path.append(\"/content/deforestation-risk-gialai\")  # IMPORTANT\n",
        "\n",
        "from src.pipeline import run_pipeline\n",
        "\n",
        "run_pipeline(\n",
        "  \"/content/KBang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/KBang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/outputs\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Gaan5v_QLeg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "zip_path = shutil.make_archive(\"/content/outputs\", \"zip\", \"/content/outputs\")\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "lXoKcWV5Lkl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Borino88/deforestation-risk-gialai.git\n",
        "%cd deforestation-risk-gialai\n",
        "!pip -q install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "8Zx70VlFLv_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "print(\"Uploaded:\", list(uploaded.keys()))\n"
      ],
      "metadata": {
        "id": "4orOlg9keiLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from src.pipeline import run_pipeline\n",
        "\n",
        "out_dir = \"/content/outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "run_pipeline(\n",
        "  \"/content/KBang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/KBang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  out_dir\n",
        ")\n",
        "\n",
        "print(\"DONE. Outputs saved to:\", out_dir)\n"
      ],
      "metadata": {
        "id": "eglPfvEpelxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from src.pipeline import run_pipeline\n",
        "\n",
        "out_dir = \"/content/outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "run_pipeline(\n",
        "  \"/content/KBang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/KBang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TRAIN_master_rain_elev_lossyear.csv\",\n",
        "  \"/content/MangYang_TEST_master_rain_elev_lossyear.csv\",\n",
        "  out_dir\n",
        ")\n",
        "\n",
        "print(\"DONE. Outputs saved to:\", out_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "me_VX3A8e8-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "zip_path = shutil.make_archive(\"/content/outputs_package\", \"zip\", \"/content/outputs\")\n",
        "print(\"Created:\", zip_path)\n",
        "files.download(zip_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "VRiUdrNugcmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TCwAF5pDgdn8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}